# üê≥ Ollama Setup f√ºr lokale Modelle

## **Warum Ollama?**
- **Datenschutz**: Alle Daten bleiben lokal
- **Kostenlos**: Keine API-Kosten
- **Schnell**: Keine Netzwerk-Latenz
- **Offline**: Funktioniert ohne Internet

## **Installation:**

### **Windows:**
1. Besuchen Sie: https://ollama.ai/download
2. Laden Sie den Windows-Installer herunter
3. F√ºhren Sie die Installation aus
4. Starten Sie Ollama

### **macOS:**
```bash
brew install ollama
ollama serve
```

### **Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
```

## **Modelle herunterladen:**

### **Empfohlene Modelle:**
```bash
# F√ºr Code-Generierung
ollama pull qwen2.5-coder:32b-instruct

# F√ºr allgemeine Aufgaben
ollama pull llama3.3:70b-instruct

# F√ºr schnelle Antworten
ollama pull phi:latest
```

## **Testen:**
```bash
# Testen Sie das Modell
ollama run qwen2.5-coder:32b-instruct "Schreibe eine Hello World Funktion in Python"
```

## **Guido Integration:**
1. Ollama l√§uft auf `http://127.0.0.1:11434`
2. Guido erkennt automatisch verf√ºgbare Modelle
3. Verwenden Sie "Nur lokale Modelle" f√ºr Datenschutz

## **Troubleshooting:**
- **Port 11434 blockiert**: Firewall-Einstellungen pr√ºfen
- **Nicht genug RAM**: Kleinere Modelle verwenden
- **Langsam**: GPU-Beschleunigung aktivieren
