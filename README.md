# Model Router f√ºr VSCode/Cursor

Eine intelligente VSCode-Extension, die **automatisch das optimale KI-Modell** f√ºr jede Aufgabe ausw√§hlt. Mit Unterst√ºtzung f√ºr OpenAI, DeepSeek, Grok, Phi, Ollama und anderen Providern.

## üéØ Features

### ‚ö° Intelligentes Routing
- **Automatische Modellauswahl** basierend auf Prompt-Inhalt, Dateityp und Kontext
- **Regelbasiertes System** mit anpassbaren Routing-Regeln
- **Optionaler LLM-Classifier** f√ºr erweiterte Prompt-Analyse
- **Fallback-Mechanismen** bei Ausf√§llen oder Rate Limits

### üåê Multi-Provider-Unterst√ºtzung
- **OpenAI** (GPT-4o, GPT-4o-mini, GPT-4.1)
- **DeepSeek** (v3, r1 Reasoning)
- **Grok** (xAI)
- **Microsoft Phi** (4, 4-mini)
- **Ollama** (lokale Modelle: Llama, Qwen, CodeLlama)
- **Beliebige OpenAI-kompatible APIs**

### üí∞ Kostenbewusstsein
- **Live-Kostensch√§tzung** vor Ausf√ºhrung
- **Budget-Management** mit Tages- und Monatslimits
- **Ausgaben-Tracking** und Statistiken
- **Preisvergleich** zwischen Modellen

### üîí Sicherheit & Datenschutz
- **Sichere API-Key-Speicherung** √ºber VSCode SecretStorage
- **Privacy-Modi**: `privacy-strict`, `local-only`, `offline`
- **Datenschutz-Filter** f√ºr sensible Dateipfade
- **Keine Klartext-Speicherung** von Geheimnissen

### üé® Benutzerfreundlichkeit
- **Statusbar-Integration** mit aktuellem Modus
- **QuickPick-Men√ºs** f√ºr Provider/Modell-Override
- **Output-Channel** f√ºr detaillierte Logs
- **Command Palette** Integration

## üöÄ Installation

### 1. Voraussetzungen
- VSCode 1.90.0 oder neuer
- Node.js 20+ (f√ºr Entwicklung)
- Optional: Ollama f√ºr lokale Modelle

### 2. Extension installieren

#### Entwicklungsversion (empfohlen)
```bash
# Repository klonen
git clone <repository-url>
cd model-router

# Dependencies installieren
npm install

# TypeScript kompilieren
npm run compile

# Extension in VSCode laden
# F5 dr√ºcken oder "Run Extension" in der Debug-Ansicht
```

#### Aus Package installieren
```bash
# Extension packen
npm run package

# In VSCode installieren: Ctrl+Shift+P -> "Extensions: Install from VSIX"
```

### 3. Konfiguration einrichten

Die Extension erstellt automatisch eine Standard-Konfiguration:
```
workspace/
‚îú‚îÄ‚îÄ router.config.yaml    # Haupt-Konfiguration
‚îî‚îÄ‚îÄ .vscode/
    ‚îî‚îÄ‚îÄ settings.json     # VSCode-Einstellungen
```

## ‚öôÔ∏è Konfiguration

### Basis-Konfiguration (router.config.yaml)

```yaml
version: 1
activeProfile: default

profiles:
  default:
    mode: auto  # auto|speed|quality|cheap|local-only|privacy-strict
    
    budget:
      dailyUSD: 2.50
      hardStop: true
      warningThreshold: 80
    
    privacy:
      redactPaths: ["**/secrets/**", "**/.env*"]
      stripFileContentOverKB: 256
      allowExternal: true
    
    providers:
      # OpenAI
      - id: openai
        kind: openai-compat
        baseUrl: https://api.openai.com/v1
        apiKeyRef: OPENAI_API_KEY
        models:
          - name: gpt-4o-mini
            context: 128000
            caps: ["cheap","tools","json"]
            price:
              inputPerMTok: 0.15
              outputPerMTok: 0.60
      
      # Lokale Modelle via Ollama
      - id: ollama
        kind: ollama
        baseUrl: http://127.0.0.1:11434
        models:
          - name: llama3.3:70b-instruct
            context: 32768
            caps: ["local","long","tools"]
    
    routing:
      rules:
        # Tests ‚Üí g√ºnstige Modelle
        - id: cheap-tests
          if:
            anyKeyword: ["test", "unit test", "boilerplate"]
            fileLangIn: ["ts","js","py"]
          then:
            prefer: ["openai:gpt-4o-mini"]
            target: chat
        
        # Privacy ‚Üí lokal erzwingen
        - id: privacy
          if:
            privacyStrict: true
          then:
            prefer: ["ollama:llama3.3:70b-instruct"]
            target: chat
      
      default:
        prefer: ["openai:gpt-4o-mini", "ollama:llama3.3:70b-instruct"]
        target: chat
```

### VSCode-Einstellungen

```json
{
  "modelRouter.configPath": "${workspaceFolder}/router.config.yaml",
  "modelRouter.mode": "auto",
  "modelRouter.enablePromptClassifier": false
}
```

## üîë API-Keys einrichten

### Methode 1: Command Palette
1. `Ctrl+Shift+P` ‚Üí "Model Router: Set API Key"
2. Provider-ID eingeben (z.B. `openai`)
3. API-Key eingeben

### Methode 2: Umgebungsvariablen
```bash
# .env oder System-Umgebung
export OPENAI_API_KEY="sk-..."
export DEEPSEEK_API_KEY="sk-..."
export GROK_API_KEY="..."
```

Dann: `Ctrl+Shift+P` ‚Üí "Model Router: Import API Keys"

### Unterst√ºtzte Provider

| Provider | API-Key Format | Basis-URL |
|----------|----------------|-----------|
| OpenAI | `sk-...` | `https://api.openai.com/v1` |
| DeepSeek | `sk-...` | `https://api.deepseek.com/v1` |
| Grok (xAI) | varies | `https://api.x.ai/v1` |
| Phi | varies | provider-specific |
| Ollama | none | `http://127.0.0.1:11434` |

## üéÆ Nutzung

### Chat-Interface
```
Ctrl+Shift+P ‚Üí "Model Router: Chat"
```
- Prompt eingeben
- Extension w√§hlt automatisch das beste Modell
- Live-Kostensch√§tzung und Begr√ºndung
- Streaming-Response im Output-Channel

### Einmaliges Routing
```
Ctrl+Shift+P ‚Üí "Model Router: Route Prompt Once"
```
- Text im Editor ausw√§hlen
- Modellvorschlag ohne Ausf√ºhrung
- Zeigt Score und Begr√ºndung

### Modi wechseln
Klick auf Statusbar oder:
```
Ctrl+Shift+P ‚Üí "Model Router: Switch Mode"
```

**Verf√ºgbare Modi:**
- `auto` ‚Äì Intelligente Auswahl basierend auf Kontext
- `speed` ‚Äì Schnellste verf√ºgbare Modelle
- `quality` ‚Äì Hochwertigste Modelle (teurer)
- `cheap` ‚Äì G√ºnstigste Modelle
- `local-only` ‚Äì Nur lokale Modelle (Ollama)
- `privacy-strict` ‚Äì Strenger Datenschutz, keine externen APIs

### Weitere Commands

| Command | Beschreibung |
|---------|--------------|
| `Model Router: Open Config` | Konfigurationsdatei √∂ffnen |
| `Model Router: Show Costs` | Ausgaben-√úbersicht |
| `Model Router: Test Connection` | Provider-Verbindungen testen |
| `Model Router: Classify Prompt` | Prompt-Klassifikation anzeigen |
| `Model Router: Simulate Routing` | Routing simulieren |

## üìä Routing-Regeln

### Regel-Struktur

```yaml
- id: regel-name
  if:
    anyKeyword: ["test", "debug"]      # Enth√§lt eines dieser W√∂rter
    allKeywords: ["fix", "bug"]        # Enth√§lt alle diese W√∂rter  
    fileLangIn: ["ts", "js", "py"]     # Dateityp
    filePathMatches: ["**/test/**"]    # Pfad-Pattern
    minContextKB: 128                  # Mindest-Dateigr√∂√üe
    maxContextKB: 512                  # Maximal-Dateigr√∂√üe
    privacyStrict: true                # Privacy-Modus
    mode: ["quality", "speed"]         # Aktiver Modus
  then:
    prefer: ["provider:model", "..."]  # Bevorzugte Modelle
    target: chat                       # Ziel-Art
    priority: 10                       # Zus√§tzliche Priorit√§t
```

### Beispiel-Regeln

```yaml
# Unit Tests ‚Üí g√ºnstige Modelle
- id: unit-tests
  if:
    anyKeyword: ["test", "spec", "jest", "pytest"]
    fileLangIn: ["ts", "js", "py"]
  then:
    prefer: ["openai:gpt-4o-mini", "phi:phi-4-mini"]
    target: chat

# Komplexe Algorithmen ‚Üí Reasoning-Modelle  
- id: complex-algorithms
  if:
    anyKeyword: ["algorithm", "complexity", "optimize", "prove"]
  then:
    prefer: ["deepseek:deepseek-r1", "openai:gpt-4.1"]
    target: chat

# Gro√üe Refactorings ‚Üí Long-Context
- id: large-refactor
  if:
    minContextKB: 200
    anyKeyword: ["refactor", "restructure"]
  then:
    prefer: ["openai:gpt-4.1", "phi:phi-4"]
    target: chat

# Privacy-Mode ‚Üí nur lokal
- id: privacy-only
  if:
    privacyStrict: true
  then:
    prefer: ["ollama:llama3.3:70b-instruct"]
    target: chat
```

## üí∞ Budget-Management

### Konfiguration
```yaml
budget:
  dailyUSD: 5.0        # Tagesdeckel
  monthlyUSD: 100.0    # Monatsdeckel  
  hardStop: true       # Bei √úberschreitung stoppen
  warningThreshold: 80 # Warnung bei 80% Verbrauch
```

### Kosten-Tracking
- Automatische Kostenerfassung nach jeder API-Anfrage
- Aufschl√ºsselung nach Provider und Modell
- Export f√ºr externe Analyse
- Budget-Warnungen und Hard-Stops

### Commands
```
Model Router: Show Costs     # Ausgaben-√úbersicht
```

Zeigt:
- Tagesausgaben vs. Budget
- Monatsausgaben vs. Budget  
- Transaktionsanzahl
- Top-Provider und -Modelle
- Ausgaben-Trend

## üîí Datenschutz & Sicherheit

### API-Key-Sicherheit
- **VSCode SecretStorage**: Nutzt System-Keychain (Windows Credential Manager, macOS Keychain, Linux Secret Service)
- **Keine Klartext-Speicherung**: Keys werden nie in Konfigurationsdateien gespeichert
- **Umgebungsvariablen-Fallback**: Sichere Alternative f√ºr Team-Setups

### Privacy-Modi

**`privacy-strict` Modus:**
- Erzwingt lokale Modelle (Ollama)
- Blockiert alle externen API-Calls
- Redaktiert sensible Dateipfade
- Strippt gro√üe Dateien automatisch

**Datenschutz-Filter:**
```yaml
privacy:
  redactPaths: 
    - "**/secrets/**"
    - "**/.env*" 
    - "**/credentials/**"
  stripFileContentOverKB: 256
  allowExternal: false  # Bei privacy-strict
```

### Offline-Betrieb
```yaml
mode: offline  # oder local-only
```
- Nur Ollama-Provider aktiv
- Keine Internetverbindung erforderlich
- Vollst√§ndig lokale Verarbeitung

## üõ†Ô∏è Entwicklung

### Setup
```bash
git clone <repo>
cd model-router
npm install
npm run compile
```

### Struktur
```
src/
‚îú‚îÄ‚îÄ extension.ts           # VSCode Extension Entry Point
‚îú‚îÄ‚îÄ config.ts             # YAML-Konfiguration  
‚îú‚îÄ‚îÄ router.ts             # Routing-Engine
‚îú‚îÄ‚îÄ secret.ts             # API-Key-Management
‚îú‚îÄ‚îÄ price.ts              # Kostenberechnung
‚îú‚îÄ‚îÄ promptClassifier.ts   # KI-Klassifikation
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ base.ts          # Provider-Interface
‚îÇ   ‚îú‚îÄ‚îÄ openaiCompat.ts  # OpenAI-kompatibel
‚îÇ   ‚îî‚îÄ‚îÄ ollama.ts        # Lokale Modelle
‚îî‚îÄ‚îÄ mcp/
    ‚îî‚îÄ‚îÄ server.ts        # MCP-Integration
```

### Testing
```bash
# Extension in Development Host starten
npm run watch    # Auto-compile
F5              # Launch Extension Development Host

# Provider-Verbindungen testen
Ctrl+Shift+P ‚Üí "Model Router: Test Connection"

# Routing-Simulation
Ctrl+Shift+P ‚Üí "Model Router: Simulate Routing"
```

### Neue Provider hinzuf√ºgen
1. Provider-Klasse in `src/providers/` erstellen
2. `BaseProvider` erweitern
3. In `extension.ts` registrieren
4. Konfiguration in `router.config.yaml` erg√§nzen

## üîß Problembehandlung

### H√§ufige Probleme

**"Router nicht initialisiert"**
- Konfigurationsdatei pr√ºfen: `Model Router: Open Config`
- API-Keys setzen: `Model Router: Set API Key`
- Extension neu laden: `Developer: Reload Window`

**"Provider nicht verf√ºgbar"**
- Verbindung testen: `Model Router: Test Connection`
- API-Key pr√ºfen
- Firewall/Proxy-Einstellungen √ºberpr√ºfen

**"Kein passendes Modell gefunden"**
- Routing-Regeln in Konfiguration pr√ºfen
- Fallback-Modelle definieren
- Provider-Verf√ºgbarkeit testen

**Ollama-Probleme**
```bash
# Ollama installieren
curl -fsSL https://ollama.ai/install.sh | sh

# Modell laden
ollama pull llama3.3:70b-instruct
ollama pull qwen2.5-coder:32b-instruct

# Server starten
ollama serve
```

### Logs aktivieren
Output-Channel √∂ffnen: "View" ‚Üí "Output" ‚Üí "Model Router"

### Debug-Mode
```json
{
  "modelRouter.debug": true
}
```

## ü§ù Beitragen

### Issues
- Bug-Reports mit reproduzierbaren Schritten
- Feature-Requests mit Use-Cases
- Provider-spezifische Probleme mit Logs

### Pull Requests
1. Fork erstellen
2. Feature-Branch: `git checkout -b feature/neue-funktion`
3. Tests hinzuf√ºgen
4. PR erstellen mit Beschreibung

### Provider-Requests
Neue Provider k√∂nnen angefragt werden. Ben√∂tigt:
- API-Dokumentation
- Preismodell
- Beispiel-API-Keys (f√ºr Tests)

## üìã Roadmap

### v0.2.0
- [ ] Webview-Chat-Interface
- [ ] Tool-Calling-Unterst√ºtzung
- [ ] Vollst√§ndige MCP-Integration
- [ ] Team-Konfigurationen

### v0.3.0
- [ ] Anthropic Claude Support
- [ ] Azure OpenAI Integration
- [ ] Performance-Benchmarking
- [ ] A/B-Testing f√ºr Modelle

### v0.4.0
- [ ] Workflow-Automatisierung
- [ ] GitHub Copilot Integration
- [ ] Enterprise-Features
- [ ] Telemetrie-Dashboard

## üìÑ Lizenz

MIT License - siehe [LICENSE](LICENSE) f√ºr Details.

## üôè Danksagungen

- VSCode Team f√ºr die Extension API
- OpenAI, DeepSeek, xAI f√ºr die APIs
- Ollama Community f√ºr lokale Modelle
- MCP-Protokoll Contributors

---

**Weitere Fragen?** √ñffne ein [Issue](../../issues) oder schaue in die [Diskussionen](../../discussions).
